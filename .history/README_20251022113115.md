# Graph-TempCZ: Software Mentions and Re-use in Scientific Publications

Graph-TempCZ is a large-scale heterogeneous graph dataset and experiment suite for studying how scientific publications mention and re-use software over time. It includes GNN-based link prediction, topology-driven heuristics, hybrid methods, and temporal generalization across publication years.

<!-- Optional figures
![Top 10 software overall](top10software.png)
![Top 10 software by publication year](top_10software_of_each_year.png)
-->

## Table of Contents
- Overview
- Dataset at a glance
- Installation
- Data preparation
- Quick start
- Reproduce paper-style experiments
- Project structure
- Results (summary)
- Citation
- License & Acknowledgements
- Contact

## Overview
- Paper (reference): LREC 2026 — "GraphTempCZ: A Graph-Based Perspective on Software Usage and Evolution in Scientific Publications". Link TBD.
- Goal: Predict publication–software mention links using heterogeneous graph signals, text embeddings, and topology cues, and test temporal robustness across years.
- Highlights:
   - 6M+ mention edges between publications and software entities spanning 1959–2022.
   - Sentence-transformer embeddings fused with PyTorch Geometric heterogeneous GNNs.
   - Strong topology-only and hybrid baselines alongside embedding-enhanced GNNs.
   - Temporal split utilities and caching to evaluate cross-year generalization.

## Dataset at a glance
- Nodes: publications, software entities (plus attributes such as year, title/name, etc.).
- Edges: mention links (publication → software) with timestamps/years.
- Files expected by scripts (CSV):
   - `single_graph_merged_data.csv` — raw publication–software mention graph.
   - `single_graph_merged_data_label.csv` — labeled graph for supervised training (pos/neg).

## Installation
Recommended: Python 3.9+ and a virtual environment.

```bash
python -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install torch torchvision torchaudio \
      torch-geometric torch-sparse torch-scatter torch-cluster torch-spline-conv \
      sentence-transformers pandas numpy scikit-learn tqdm wandb matplotlib seaborn \
      networkx psutil xgboost
```

Note: Install the exact PyTorch and PyG wheels matching your CUDA setup from the official docs if using GPUs.

## Data preparation
1) Place the CSVs where scripts can find them. By default we assume a layout one level above the repo:

```
Graph-TempCZ/
└── ../datasets/
      ├── single_graph_merged_data.csv
      └── single_graph_merged_data_label.csv
```

2) If your layout differs, adjust the `DATA_PATH` constants inside the scripts accordingly.

3) (Optional) See `1.pdf`–`3.pdf` for sampling strategies, temporal splits, and evaluation settings.

## Quick start
Ensure the environment and data paths are set (see above), then try:

```bash
# Embedding-enhanced GNN baseline (logs AUC/AP/F1/Recall, saves predictions)
python GNN.py

# Temporal single-year train with cached embeddings; evaluate across years
python temporal_link_prediction.py

# Topology-only and hybrid baselines
python feature_embedding_link_prediction.py
python topology_embedding_link_prediction.py
python topology_and_embedding_combination_link_prediction.py
```

Tips
- Set `WANDB_MODE=offline` for local-only logging.
- Control GPU selection via `CUDA_VISIBLE_DEVICES` (falls back to CPU if no CUDA).
- Temporal pipeline uses cache dirs like `./single_year_split_cache_refine_2020_2021/`.

## Reproduce paper-style experiments
The scripts in this repo correspond to the study's main variants:
- GNN-based link prediction with sentence-transformer embeddings (`GNN.py`).
- Temporal single-year training and multi-year testing with aggressive caching (`temporal_link_prediction.py`).
- Baselines combining embedding features and classical topology heuristics (`feature_embedding_link_prediction.py`, `topology_embedding_link_prediction.py`, `topology_and_embedding_combination_link_prediction.py`).

General notes
- Ensure your CSVs conform to the expected schema (IDs for publication/software, year, labels for supervised setups).
- Cold-start and temporal shifts are evaluated by training on a target year and testing on other years via `temporal_link_prediction.py`.

## Project structure
```
.
├── GNN.py                                  # Embedding-enhanced GNN link prediction
├── temporal_link_prediction.py             # Temporal split training & evaluation with caching
├── feature_embedding_link_prediction.py    # Embedding + classical ML baselines
├── topology_embedding_link_prediction.py   # Topology heuristics + models (e.g., XGBoost)
├── topology_and_embedding_combination_link_prediction.py  # Fused topology + embedding features
├── environment.yml                         # Optional conda environment (if provided)
├── README.md                               # This file
├── fig/                                    # Figures (e.g., top software visualizations)
└── spider/
      └── spider_full.py                      # Data collection / processing utilities (if applicable)
```

## Results (summary)
- Scripts print metrics to console and, when available, log to Weights & Biases.
- Prediction CSVs and intermediate artifacts are saved by the respective scripts (see their in-file arguments/paths).
- For full tables/plots as in the paper, follow the temporal evaluation flow and aggregate results per year.

## Citation
If you use Graph-TempCZ, please cite the paper:

```
@inproceedings{graph-tempcz-2026,
   title     = {GraphTempCZ: A Graph-Based Perspective on Software Usage and Evolution in Scientific Publications},
   author    = {Authors TBD},
   booktitle = {Proceedings of LREC 2026},
   year      = {2026},
   note      = {Project: Graph-TempCZ. URL: https://github.com/caocongfeng/Graph-TempCZ}
}
```

Replace authors/venue details with the final citation once available.

## License & Acknowledgements
- License: add your chosen license here (e.g., MIT/Apache-2.0). If absent, this code is provided for research purposes.
- Acknowledgements: Built with PyTorch, PyTorch Geometric, sentence-transformers, scikit-learn, XGBoost, and related open-source tooling.

## Contact
For questions or issues, please open a GitHub issue or contact the maintainers.
